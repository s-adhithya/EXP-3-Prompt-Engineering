# EXP-3-PROMPT-ENGINEERING-

## Aim: 
Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta
Experiment:
Within a specific use case (e.g., summarizing text, answering technical questions), compare the performance, user experience, and response quality of prompting tools across these different AI platforms.

## Algorithm:
1. Define tasks & metrics (faithfulness, format adherence, citations, latency, cost).
2. Prepare standardized inputs & prompts (same article, same technical question).
3. Run each model with fixed settings; log outputs and metadata.
4. Score with a rubric; verify claims/citations; compute aggregate stats.
5. Synthesize findings into a structured report with summaries, templates, and recommendations.
## Prompt
Write a professional report comparing 2024 prompting tools across ChatGPT, Claude, Bard/Gemini, Cohere Command, and Meta (Llama). Focus on two use cases: summarizing a long technical article and answering a moderately hard technical question. Include methodology, evaluation criteria, key strengths/weaknesses, detailed comparisons, recommendations, and example prompt templates. Keep the tone analytical, structured with clear headings and bullet points where needed, and avoid marketing language.
## Output
[lab.3.Evaluation.of.2024.Prompting.Tools.Across.pdf](https://github.com/user-attachments/files/22131113/lab.3.Evaluation.of.2024.Prompting.Tools.Across.pdf)

## Result
The report was successfully submitted.
